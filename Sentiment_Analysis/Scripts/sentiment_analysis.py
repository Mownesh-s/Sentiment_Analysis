# -*- coding: utf-8 -*-
"""Sentiment_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13zk8oDXOsWSMd03XVHGjnW4bzp2ms53-
"""

pip install datasets

"""# Importing Necessary Libraries"""

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from datasets import load_dataset
import os

"""# Using Dataset From Datasets Library"""

dataset = load_dataset("yelp_review_full")

"""# Spliting The Data"""

train_texts = [x["text"] for x in dataset["train"]]
train_labels = np.array([x["label"] for x in dataset["train"]])
test_texts = [x["text"] for x in dataset["test"]]
test_labels = np.array([x["label"] for x in dataset["test"]])

def categorize_sentiment(label):
    if label in [0, 1]:
        return 0  # Negative
    elif label == 2:
        return 1  # Neutral
    else:
        return 2  # Positive

"""# Preprocessing the Data"""

train_labels = np.array([categorize_sentiment(label) for label in train_labels])
test_labels = np.array([categorize_sentiment(label) for label in test_labels])

vocab_size = 20000
max_length = 300
tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
tokenizer.fit_on_texts(train_texts)
train_sequences = tokenizer.texts_to_sequences(train_texts)
test_sequences = tokenizer.texts_to_sequences(test_texts)
train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')
test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')

"""# Downloading GloVe(Preprocessed Embedding)"""

!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip -q glove.6B.zip

"""# Using GloVe"""

def load_glove_embeddings(glove_path="glove.6B.100d.txt", embedding_dim=100):
    embeddings_index = {}
    with open(glove_path, encoding="utf-8") as f:
        for line in f:
            values = line.split()
            word = values[0]
            coefs = np.asarray(values[1:], dtype="float32")
            embeddings_index[word] = coefs
    return embeddings_index

embedding_dim = 100
glove_embeddings = load_glove_embeddings()

embedding_matrix = np.zeros((vocab_size, embedding_dim))
for word, i in tokenizer.word_index.items():
    if i < vocab_size:
        embedding_vector = glove_embeddings.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

"""# Bidirectional Lstm Model"""

model = Sequential([
    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False),
    Bidirectional(LSTM(128, return_sequences=True)),
    Dropout(0.3),
    Bidirectional(LSTM(64)),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(3, activation='softmax')
])

model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4), metrics=['accuracy'])
model.summary()

early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

"""# Training the model"""

history = model.fit(train_padded, train_labels, epochs=15, validation_data=(test_padded, test_labels), batch_size=128, callbacks=[early_stopping])

"""# Test Accuracy"""

test_loss, test_accuracy = model.evaluate(test_padded, test_labels)
print(f"Test Accuracy: {test_accuracy:.4f}")

"""# Ploting for Accuracy and Loss"""

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.title('Accuracy')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.title('Loss')
plt.show()

"""# Function for Prediction"""

def predict_sentiment(text):
    seq = tokenizer.texts_to_sequences([text])
    padded = pad_sequences(seq, maxlen=max_length, padding='post', truncating='post')
    pred = model.predict(padded)
    labels = ['Negative', 'Neutral', 'Positive']
    return labels[np.argmax(pred)]

"""# Sample Prediction"""

sample_reviews = [
    "The food was amazing and the service was excellent!",
    "It was okay, not great but not terrible either.",
    "Worst experience ever. I will never come back!"
]

for review in sample_reviews:
    print(f"Review: {review} -> Sentiment: {predict_sentiment(review)}")

"""# Saving the model"""

import pickle
model.save("sentiment_model.h5")

with open("tokenizer.pkl", "wb") as f:
    pickle.dump(tokenizer, f)

